{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Example in Excel\n",
    "\n",
    "<video controls src=\"regression.mp4\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The short video show a verey simple example of Linear Regression using Excel.\n",
    "\n",
    "The values for the feature and taget are all generated using random numbers, however a loose relationship between the two (i.e how thay are calculatted has been maintained.\n",
    "\n",
    "The values are re-generated by hitting F9\n",
    "\n",
    "In a real Linear Regression situation there are likely to be many features, not just one. \n",
    "\n",
    "What our simple example has in common with a more realistic case is that we would be aiming to predict\n",
    "the target value from a set of features (in this case a set of one)\n",
    "\n",
    "Excel illustrates how it would make the prediction by inserting a 'best fit' line through the points. \n",
    "We will discuss best fit a bit later.\n",
    "\n",
    "You can also have Excel write out the equation of the line it has drawn. This is essentially the Linear Regression model that \n",
    "it has calculated.\n",
    "\n",
    "Two things should be immediately obvious;\n",
    "\n",
    "1. Almost non-of the real points lay on the line\n",
    "2. As the data points change, the line and the equation changes\n",
    "\n",
    "\n",
    "**Conclusion: Exercise!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "In order to get Excel to produce a model at all we neded to provide both feature(s) values and target values\n",
    "\n",
    "That is we need to provide features for which we already know the answers. By providing both the feateres and the associated Target values in this way, we allow a model to be trained so as to predict target values (which we didn't know) from new set of features which we did. \n",
    "\n",
    "This is called Supervised learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset \n",
    "\n",
    "For this lesson we will use a dataset which is included with the scikit package.\n",
    "\n",
    "The dataset that we will be using is the **Boston House prices** dataset.\n",
    "\n",
    "The datasets in scikit are provided as Dictionary objects. This allows both the data and appropriate metadata, including provenance and citation information to be included.\n",
    "\n",
    "You can see the contents of the dictionary with the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset  - Boston house-prices from sklearn\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "boston_data = datasets.load_boston()\n",
    "\n",
    "#print(boston_data)\n",
    "\n",
    "#print(boston_data['DESCR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can put the data (the features) into a dataframe and add the 'target values'\n",
    "df_boston = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "df_boston['target'] = pd.Series(boston_data.target)\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how big is the dataset?\n",
    "df_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats on the numerical values\n",
    "df_boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "\n",
    "Most machine learning algorithms don't like missing data. \n",
    "\n",
    "In this particular case we don't have any, but if there was we could adopt standard approaches to either removing such rows or imputing the missing values.\n",
    "\n",
    "The following code is just examples of what you might do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = pd.read_csv(\"MissingData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The missing values in a dataframe are represented by 'NaN' \n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can 'drop' all of the rows containing a NaN with\n",
    "\n",
    "print(df_missing.shape)\n",
    "df_missing.dropna(inplace=True)\n",
    "print(df_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 35% reduction in data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = pd.read_csv(\"MissingData.csv\")\n",
    "df_missing = df_missing.fillna(df_missing.mean())\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = pd.read_csv(\"MissingData.csv\")\n",
    "df_missing['CatA'].fillna('Unknown', inplace = True)\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of the data\n",
    "\n",
    "We are looking for insights as to the nature of the data as a whole.  We can use different visualizations for different types of data.\n",
    "\n",
    "We will use matplotlib for our visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need the pyplot functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# needed by jupyter to ensure that the plots appear inline (in the usual output cell)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remind ourselves what our dataset looks like\n",
    "df_boston.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can create simple plots to look at the  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both histograms ...\n",
    "for col in df_boston.columns:\n",
    "    df_boston[col].hist(bins = 20)\n",
    "    plt.title('Histogram of ' + col)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ...   and boxplots can be useful\n",
    "\n",
    "for col in df_boston.columns:\n",
    "    df_boston.boxplot(column = col)\n",
    "    plt.title('Boxplot of ' + col)\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable correlations\n",
    "import seaborn as sns\n",
    "sns.pairplot(df_boston)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at the correlation between each pair of variables\n",
    "\n",
    "corr = df_boston.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or graphically with a heatmap \n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "heat_map = sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with outliers\n",
    "\n",
    "[Demonstration using Excel of the effect of Outliers ]\n",
    "\n",
    "<video controls src=\"regression_outliers.mp4\" />\n",
    "\n",
    "From the demonstration you can see that outliers can distort considerably the position of the trendline. This is not desirable.\n",
    "\n",
    "For a given set of values there is no real definition of which are outliers. A common approach is to consider any value outside of 2 standard deviations of the mean could be considered an outlier.\n",
    "\n",
    "We will adopt this approach and write a function which will list all of the rows in the dataset which contain such values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a list of the columns which could contain outliers.\n",
    "# In our dataset it is all of the columns with the exception of the Target column and the Chas\n",
    "# column which we know from the description is a categorical boolean value.\n",
    "\n",
    "poss_outlier_columns = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go back and look at the .describe values for the CHAS column you can see that if we included it in this approach, we would effectively remove all of the 1 valued rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common approach to removing outliers is to treat all data outside of 2 standard deviations of the mean as outliers\n",
    "# We will create a small function to do this and then passs it our dataframe and our list of columns\n",
    "\n",
    "def get_outliers(data, columns) :\n",
    "    # create a list for the results\n",
    "    outlier_list = []\n",
    "    for col in columns:\n",
    "        mean = data[col].mean()\n",
    "        sd = data[col].std()\n",
    "        # get the index values of all values higher or lower than the mean +/- 2 standard deviations\n",
    "        outliers = data[(data[col] > mean + 2*sd) | (data[col]  < mean  - 2*sd)].index\n",
    "        # and add those values to our list\n",
    "        outlier_list  += [x for x in outliers]\n",
    "        # put our list into a set, as this will remove duplicates\n",
    "        # and then return it as a list\n",
    "    return list(set(outlier_list))\n",
    "\n",
    "# creat our list of outlier row indexes\n",
    "boston_outliers = get_outliers(df_boston, poss_outlier_columns)\n",
    "\n",
    "# and then drop them\n",
    "df_boston = df_boston.drop(boston_outliers, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets repeat some of the graphics\n",
    "sns.pairplot(df_boston)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and\n",
    "corr = df_boston.corr()\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "heat_map = sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the corr figures\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# this function loops through columns in a data set and defines a predefined scaler to each\n",
    "def scale_numeric(data, numeric_columns, scaler):\n",
    "    for col in numeric_columns:\n",
    "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "    return data\n",
    "\n",
    "# we can now define the scaler we want to use and apply it to our dataset \n",
    "\n",
    "# Other scalers are available see the scikit documentation\n",
    "scaler = StandardScaler()\n",
    "df_boston = scale_numeric(df_boston, poss_outlier_columns, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far all we have been doing is cleaning and preparing the data to make it more acceptable to the algorithm we want to use.\n",
    "\n",
    "Now we need to make sure that we have suitable data to both create the model and some data to test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "\n",
    "Currently our dataset includes all of the (remaining) rows and each row includes the target column. I.e. the values that we would like the model to predict.\n",
    "\n",
    "For a Supervised learning method like Regression, we need to provide the learning algorithm with both the predictor columns along with the corresponding target values to enable the model to create and train itself.\n",
    "\n",
    "We also need to keep some of the dataframe rows back so that after we have created the model we have available data with which to test it\n",
    "\n",
    "\n",
    "For the names used for the these new dataframes we follow convention and use 'X' to indicate the predictors and 'y' for the target (the predicted).  So we will end up with 4 distinct structures X_train, X_test, y_train and y_test.\n",
    "\n",
    "Because the need to perform this operation is so common in these supervised learning methods Scikit has its own function to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to split out the predictors from the targets and put them  into seperate dataframes.\n",
    "\n",
    "# the predictors\n",
    "df_boston_X = pd.DataFrame(df_boston,columns=boston_data.feature_names)\n",
    "df_boston_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tagets\n",
    "df_boston_y = pd.DataFrame(df_boston,columns=['target'])\n",
    "df_boston_y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can use the train_test_split function from sklearn\n",
    "\n",
    "We need to provide both the predictors and the target dataframes\n",
    "We also provide a 'test_size' value to indicate the % of the rows to be used for the test datframe. \n",
    "\n",
    "Essentially we are going to split up our original dataset into four areas\n",
    "\n",
    "\n",
    "<img src=\"Data_split.png\" />\n",
    "\n",
    "\n",
    "** Red area = ** Rows of training features used to generate the model\n",
    "\n",
    "** Purple area = ** Target values used to generate the model\n",
    "\n",
    "** Orange area = ** Rows of features used to test the model\n",
    "\n",
    "** Green area = ** The actual target values from the test data used to evaluate the accuracy of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(df_boston_X, df_boston_y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape of test and training sets that have been created\n",
    "print('Training Set Row Count: ', X_train.shape[0])\n",
    "print('Test Set Row Count: ', X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "The first step is deciding which algorithm to use. There are more than one Regression algorithms, but we are in fact going to use the lm model from scikit \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# we start by creating an object of the LinearRegression class\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "We now need to provide the fit function with the training predictors (X_train) \n",
    "and the known taget values (y_train) for these training predictor rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "We now have a model which we can use to predict the taget values for the test predictors (X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = lm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick check of the reults\n",
    "print(len(Y_pred))\n",
    "print(Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All that remains is to check - How good is the model?\n",
    "\n",
    "Bear in mind that this is only *one* model. Even using the same algorithm, changing the training set of predictors and targets could have resulted in a different model, which may have been better or worse than the one we have.\n",
    "\n",
    "Even so, we need to have some kind of measure of how good we think the model is, or how much confidence we are prepared to place in the model.\n",
    "\n",
    "In order to do this we need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(Y_test, Y_pred):\n",
    "    # this block of code returns all the metrics we are interested in \n",
    "    mse = metrics.mean_squared_error(Y_test, Y_pred)\n",
    "    msa = metrics.mean_absolute_error(Y_test, Y_pred)\n",
    "    r2 = metrics.r2_score(Y_test, Y_pred)\n",
    "\n",
    "    print(\"Mean squared error: \", mse)\n",
    "    print(\"Mean absolute error: \", msa)\n",
    "    print(\"R^2 : \", r2)\n",
    "    \n",
    "    # this creates a chart plotting predicted and actual \n",
    "    plt.scatter(Y_test, Y_pred)\n",
    "    plt.xlabel(\"Prices: $Y_i$\")\n",
    "    plt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\n",
    "    plt.title(\"Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$\")\n",
    "\n",
    "evaluate(y_test, Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
